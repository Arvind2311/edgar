{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_windows_1252_characters(restore_string):\n",
    "    \"\"\"\n",
    "        Replace C1 control characters in the Unicode string s by the\n",
    "        characters at the corresponding code points in Windows-1252,\n",
    "        where possible.\n",
    "    \"\"\"\n",
    "\n",
    "    def to_windows_1252(match):\n",
    "        try:\n",
    "            return bytes([ord(match.group(0))]).decode('windows-1252')\n",
    "        except UnicodeDecodeError:\n",
    "            # No character at the corresponding code point: remove it.\n",
    "            return ''\n",
    "        \n",
    "    return re.sub(r'[\\u0080-\\u0099]', to_windows_1252, restore_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the url to specific html_text file\n",
    "uri = r\"https://www.sec.gov/Archives/edgar/data/19617/0000019617-20-000257.txt\"\n",
    "\n",
    "# grab the response\n",
    "response = requests.get(uri)\n",
    "\n",
    "# pass it through the parser, in this case let's just use lxml because the tags seem to follow xml.\n",
    "soup = BeautifulSoup(response.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary that will house all filings.\n",
    "master_filings_dict = {}\n",
    "\n",
    "# let's use the accession number as the key. This \n",
    "accession_number = '0001104659-04-027382'\n",
    "\n",
    "# add a new level to our master_filing_dict, this will also be a dictionary.\n",
    "master_filings_dict[accession_number] = {}\n",
    "\n",
    "# this dictionary will contain two keys, the sec header content, and a documents key.\n",
    "master_filings_dict[accession_number]['sec_header_content'] = {}\n",
    "master_filings_dict[accession_number]['filing_documents'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sec-header>0000019617-20-000257.hdr.sgml : 20200225\n",
       "<acceptance-datetime>20200225163102\n",
       "ACCESSION NUMBER:\t\t0000019617-20-000257\n",
       "CONFORMED SUBMISSION TYPE:\t10-K\n",
       "PUBLIC DOCUMENT COUNT:\t\t240\n",
       "CONFORMED PERIOD OF REPORT:\t20191231\n",
       "FILED AS OF DATE:\t\t20200225\n",
       "DATE AS OF CHANGE:\t\t20200225\n",
       "\n",
       "FILER:\n",
       "\n",
       "\tCOMPANY DATA:\t\n",
       "\t\tCOMPANY CONFORMED NAME:\t\t\tJPMORGAN CHASE &amp; CO\n",
       "\t\tCENTRAL INDEX KEY:\t\t\t0000019617\n",
       "\t\tSTANDARD INDUSTRIAL CLASSIFICATION:\tNATIONAL COMMERCIAL BANKS [6021]\n",
       "\t\tIRS NUMBER:\t\t\t\t132624428\n",
       "\t\tSTATE OF INCORPORATION:\t\t\tDE\n",
       "\t\tFISCAL YEAR END:\t\t\t1231\n",
       "\n",
       "\tFILING VALUES:\n",
       "\t\tFORM TYPE:\t\t10-K\n",
       "\t\tSEC ACT:\t\t1934 Act\n",
       "\t\tSEC FILE NUMBER:\t001-05805\n",
       "\t\tFILM NUMBER:\t\t20651128\n",
       "\n",
       "\tBUSINESS ADDRESS:\t\n",
       "\t\tSTREET 1:\t\t383 MADISON AVENUE\n",
       "\t\tCITY:\t\t\tNEW YORK\n",
       "\t\tSTATE:\t\t\tNY\n",
       "\t\tZIP:\t\t\t10017\n",
       "\t\tBUSINESS PHONE:\t\t2122706000\n",
       "\n",
       "\tMAIL ADDRESS:\t\n",
       "\t\tSTREET 1:\t\t383 MADISON AVENUE\n",
       "\t\tCITY:\t\t\tNEW YORK\n",
       "\t\tSTATE:\t\t\tNY\n",
       "\t\tZIP:\t\t\t10017\n",
       "\n",
       "\tFORMER COMPANY:\t\n",
       "\t\tFORMER CONFORMED NAME:\tJ P MORGAN CHASE &amp; CO\n",
       "\t\tDATE OF NAME CHANGE:\t20010102\n",
       "\n",
       "\tFORMER COMPANY:\t\n",
       "\t\tFORMER CONFORMED NAME:\tCHASE MANHATTAN CORP /DE/\n",
       "\t\tDATE OF NAME CHANGE:\t19960402\n",
       "\n",
       "\tFORMER COMPANY:\t\n",
       "\t\tFORMER CONFORMED NAME:\tCHEMICAL BANKING CORP\n",
       "\t\tDATE OF NAME CHANGE:\t19920703\n",
       "</acceptance-datetime></sec-header>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# grab the sec-header tag, so we can store it in the master filing dictionary.\n",
    "sec_header_tag = soup.find('sec-header')\n",
    "\n",
    "# store the tag in the dictionary just as is.\n",
    "master_filings_dict[accession_number]['sec_header_content']['sec_header_code'] = sec_header_tag\n",
    "\n",
    "# display the sec header tag, so you can see how it looks.\n",
    "display(sec_header_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "The document 10-K was parsed.\n",
      "There was 312 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 0001104659-04-027382 were parsed and stored.\n"
     ]
    }
   ],
   "source": [
    "# initialize master document dictionary\n",
    "master_document_dict={}\n",
    "\n",
    "# Loop through each document in the filing\n",
    "for filing_document in soup.find_all('document'):\n",
    "    # document id\n",
    "    document_id = filing_document.type.find(text=True, recursive=False).strip()\n",
    "    if document_id!='10-K':\n",
    "        continue\n",
    "    # document sequence\n",
    "    document_sequence = filing_document.sequence.find(text=True, recursive=False).strip()\n",
    "    \n",
    "    # document filename\n",
    "    document_filename = filing_document.filename.find(text=True, recursive=False).strip()\n",
    "    \n",
    "    # document description\n",
    "    document_description = filing_document.description.find(text=True, recursive=False).strip()\n",
    "    \n",
    "    # insert the key\n",
    "    master_document_dict[document_id] = {}\n",
    "    \n",
    "    # add the differnt parts of the document\n",
    "    master_document_dict[document_id]['document_sequence'] = document_sequence\n",
    "    master_document_dict[document_id]['document_filename'] = document_filename\n",
    "    master_document_dict[document_id]['document_description'] = document_description\n",
    "    \n",
    "    # add document content\n",
    "    master_document_dict[document_id]['document_code'] = filing_document.extract()\n",
    "    \n",
    "    # get all the text in document\n",
    "    filing_doc_text = filing_document.find('text').extract()\n",
    "    \n",
    "    # get all thematic breaks (page breaks)\n",
    "    all_thematic_breaks = filing_doc_text.find_all('hr',{'style':'page-break-after:always'})\n",
    "    \n",
    "    # convert all the breaks into a string\n",
    "    all_thematic_breaks = [str(thematic_break) for thematic_break in all_thematic_breaks]\n",
    "    \n",
    "    # prep the document for being split\n",
    "    filing_doc_string = str(filing_doc_text)\n",
    "    \n",
    "    if len(all_thematic_breaks) > 0:\n",
    "        \n",
    "        # creates our pattern\n",
    "        regex_delimited_pattern = '|'.join(map(re.escape, all_thematic_breaks))\n",
    "        \n",
    "        # split the document along the thematic breaks\n",
    "        split_filing_string = re.split(regex_delimited_pattern, filing_doc_string)\n",
    "        \n",
    "        # store the document in the dictionary\n",
    "        master_document_dict[document_id]['pages_code'] = split_filing_string\n",
    "    \n",
    "    elif len(all_thematic_breaks)==0:\n",
    "        # store the document in the dictionary\n",
    "        master_document_dict[document_id]['pages_code'] = [filing_doc_string]\n",
    "        \n",
    "    # display some information to the user.\n",
    "    print('-'*80)\n",
    "    print('The document {} was parsed.'.format(document_id))\n",
    "    print('There was {} thematic breaks(s) found.'.format(len(all_thematic_breaks)))\n",
    "    \n",
    "\n",
    "# store the documents in the master_filing_dictionary.\n",
    "master_filings_dict[accession_number]['filing_documents'] = master_document_dict\n",
    "\n",
    "print('-'*80)\n",
    "print('All the documents for filing {} were parsed and stored.'.format(accession_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# first grab all documents\n",
    "filing_documents = master_filings_dict[accession_number]['filing_documents']\n",
    "\n",
    "# loop through each document\n",
    "for document_id in filing_documents:\n",
    "    \n",
    "    # display some info to give status updates.\n",
    "    print('-'*80)\n",
    "    print('Pulling document {} for text normilzation.'.format(document_id))\n",
    "    \n",
    "    # grab all the pages for each document\n",
    "    document_pages = filing_documents[document_id]['pages_code']\n",
    "    \n",
    "    # pages length\n",
    "    pages_length = len(document_pages)\n",
    "    \n",
    "    # initialize some dictionaries\n",
    "    repaired_pages = {}\n",
    "    \n",
    "    normalized_text = {}\n",
    "    \n",
    "    for index, page in enumerate(document_pages):\n",
    "        \n",
    "        # pass it through the parser to repair it\n",
    "        page_soup = BeautifulSoup(page, \"html5\")\n",
    "        \n",
    "        # grab the text from each page\n",
    "        page_text = page_soup.html.body.get_text(' ', strip=True)\n",
    "        \n",
    "        # normalize the text        \n",
    "        page_text_norm = restore_windows_1252_characters(unicodedata.normalize('NFKD', page_text))\n",
    "        \n",
    "        page_text_norm = page_text_norm.replace('  ',' ').replace('\\n',' ')\n",
    "        \n",
    "        page_number = index + 1\n",
    "        \n",
    "        # add the normalized text to the dictionary\n",
    "        normalized_text[page_number] = page_text_norm\n",
    "        \n",
    "        # add the reapired html code to the dictionary\n",
    "        repaired_pages[page_number] = page_soup\n",
    "        \n",
    "         # display a status to the user\n",
    "        print('Page {} of {} from document {} has had their text normalized.'.format(index + 1, pages_length, document_id))\n",
    "        \n",
    "    # add normalized text dictionary to the master filing dicitonary\n",
    "    filing_documents[document_id]['page_normalized_text'] = normalized_text\n",
    "    \n",
    "    # add the repaired html code back to the document dictionary\n",
    "    filing_documents[document_id]['pages_code'] = repaired_pages\n",
    "    \n",
    "    # define the generated page numbers\n",
    "    gen_page_numbers = list(repaired_pages.keys())\n",
    "    \n",
    "    # add the page numbers we have.\n",
    "    filing_documents[document_id]['pages_numbers_generated'] = gen_page_numbers    \n",
    "    \n",
    "    # display a status to the user.\n",
    "    print('All the pages from document {} have been normalized.'.format(document_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
